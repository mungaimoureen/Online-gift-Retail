---
title: "Online Retail Store Customer Segmentation"
author: "The Analyst"
date: "2022-08-04"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install packages
install.packages('dendextend')
install.packages('clValid')
install.packages('cluster')
install.packages('tidyr')
install.packages('fpc')
install.packages('gridExtra')
install.packages('factoextra')
install.packages('lubridate')
install.packages('dplyr')
install.packages('knitr')
install.packages('grid')
install.packages('tidyverse')
install.packages('ggplot2')
install.packages('DT')

install.packages("remotes")
remotes::install_github("Displayr/flipTime")
```
```{r}
library(readr)
```
```

```{r}
#load and Preview the Online Retail Dataset
retail_df<-read.csv('https://archive.ics.uci.edu/ml/datasets/online+retail')
```

```{r}
#Preview top 10 entries from the dataset
head(retail_df, 10)
```

```{r}
#preview last 10 entries
tail(retail_df, 10)
```

```{r}
#check the retail_df dimension
dim(retail_df)

#the dataset has 541909 rows and 8 columns
```

```{r}
#check the dataset structure
str(retail_df)

#the dataset variables have various data types:
#1. InvoiceNo, StockCode, Description, InvoiceDate and Country are of factor type
#2. Quantity and CustomerID are of Integer Type
#3. lastly, UnitPrice is of Number.
```

# 2. Data Cleaning and Preparation
```{r}
#checking for missing and null values
colSums(is.na(retail_df))

#there are 135080(CustomerID) missing values from the dataset in the CustomerID column. 
#However, we wont drop the column as its part of our main unit for analysis therefor we'll only drop rows wit missing values.
```

```{r}
#check for null entries in the dataset
any(is.null(retail_df))

#there are no null values
```

```{r}
#removing missing values
retail_df = na.omit(retail_df)
dim(retail_df)

#After removing the missing values, we have 406829 observations in the dataset.
```

```{r}
#Duplicate Values

dim(unique(retail_df))[1]

#We have 401604 unique values because we have duplicated values in invoiceNo and customerID and we need to retain the duplicates
```

```{r}
# CANCELLATIONS
# if the InvoiceNo starts with letter 'C', it indicates a cancellation
retail_df %>% 
  filter(grepl("C", retail_df$InvoiceNo)) %>% 
  summarise(Total = n())

#there are 8905 cancellations
```

```{r}
# Cancellations are not needed for the analysis so they can be removed
retail_df <- retail_df %>% 
  filter(!grepl("C", retail_df$InvoiceNo)) 
```

```{r}
# NEGATIVE QUANTITIES
# remove all rows with non-positive _Quantity_. 
retail_df  <- retail_df %>%
  filter(Quantity > 0)
```

# 2.1 Data Preparation
```{r}
#In order to perform analysis, we split the InvoiceDate into Day, Month, Year and Hour. 
#Hence, we first convert ed it to character and split the InvoiceDate records into weekOfDay, hourOfDay, month and year.

retail_df$InvoiceDate <- as.character(retail_df$InvoiceDate)
```

```{r}
# separate date and time components of invoice date
retail_df$date <- sapply(retail_df$InvoiceDate, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][1]})
retail_df$time <- sapply(retail_df$InvoiceDate, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][2]})
```

```{r}
# create month, year and hour of day variables
retail_df$month <- sapply(retail_df$date, FUN = function(x) {strsplit(x, split = '[-]')[[1]][2]})
retail_df$year <- sapply(retail_df$date, FUN = function(x) {strsplit(x, split = '[-]')[[1]][3]})
retail_df$hourOfDay <- sapply(retail_df$time, FUN = function(x) {strsplit(x, split = '[:]')[[1]][1]})
```

```{r}
#Install additional Packages and load
require(devtools)
install_github("Displayr/flipTime", force = TRUE) 
library(flipTime)
```

```{r}
#convert the date variables to the appopriate class so as to create a column of TotalSales and dayOfWeek.

retail_df$InvoiceDate <- AsDateTime(retail_df$InvoiceDate)
```

```{r}
#create Total sales column
retail_df = mutate(retail_df, TotalSales = Quantity*UnitPrice)
```

```{r}
#Identify day of week
retail_df$dayOfWeek <- lubridate::wday(retail_df$InvoiceDate,label = TRUE)
```

```{r}
#turn the appropriate variables into factors
retail_df$Country <- as.factor(retail_df$Country)
retail_df$month <- as.factor(retail_df$month)
retail_df$year <- as.factor(retail_df$year)
levels(retail_df$year) <- c(2010,2011)
hourOfDay <- as.factor(retail_df$hourOfDay)
retail_df$dayOfWeek <- as.factor(retail_df$dayOfWeek)
```

```{r}
#preview the new dataset
head(retail_df, 5)
```

```{r}
#To implement the RFM analysis, we further processed the data set by the following steps:

#1. identify the most recent date for each ID , to get the Recency data
#2. Calculated the quantity of transactions of a customer till present date, to get the Frequency data
#3. lastly, sum of Total Sales is the Monetary data.
```

```{r}
#Calculating Recency, Frequency and Monetary
max_date <- max(retail_df$InvoiceDate, na.rm = TRUE)
retail_df = mutate(retail_df, Diff = difftime(max_date, InvoiceDate, units = "days"))
retail_df$Diff <- floor(retail_df$Diff)
```

```{r}
#creating an RFM table for our analysis
RFM <- summarise(group_by(retail_df,CustomerID),Frequency = n(), Monetary = sum(TotalSales), Recency = min(Diff))
RFM$Recency <- as.numeric(RFM$Recency)
RFM$Monetary[is.na(RFM$Monetary)] <- 0
```

```{r}
#preview the new dataset
head(RFM,10)
```

# 3. Exploratory Data Analysis

## 3.1 Univariate Analysis

```{r}
# get summary  statistics of our dataset
summary(retail_df)
```

```{r}
# Top 10 most sold products sold by the online retail store
retail_df %>% 
  group_by(Description) %>% 
  summarize(count = n()) %>% 
  top_n(10, wt = count) %>%
  arrange(desc(count)) %>% 
  ggplot(aes(x = reorder(Description, count), y = count))+
  geom_bar(stat = "identity", fill = "royalblue", colour = "blue") +
  labs(x = "", y = "Top 10 Best Sellers", title = "Most Ordered Products") +
  coord_flip() +
  theme_grey(base_size = 12)


#The White hanging heart t-light holder is the most popular item orederd.
```


```{r}
#Day of the week people tend to order a lot hence more revenue
ggplot(summarise(group_by(retail_df, dayOfWeek), revenue = sum(TotalSales)), aes(x = dayOfWeek, y = revenue)) + geom_bar(stat = 'identity', fill = 'Steel Blue') + labs(x = 'Day of Week', y = 'Revenue (£)', title = 'Revenue by Day of Week') + 
  theme_minimal()

#Tuesday and Thursday are the days where more revenue were generated in comparison to other weekdays.
```

```{r}
#Transactions By hour of the Day Analysis

ggplot(summarise(group_by(retail_df, hourOfDay), transactions = n_distinct(InvoiceNo)), aes(x = hourOfDay, y = transactions)) + geom_bar(stat = 'identity', fill = "Steel Blue") + labs(x = 'Hour of Day', y = 'transactions (£)', title = 'Transactions by hour of Day') + 
  theme_minimal()

#The graph shows that between 10am till 3pm most of the orders are placed online.
```

```{r}
#Transactions By Country

Transactions_by_Country <- top_n(arrange(summarise(group_by(retail_df, Country), 'Number of Transcations' = n()), desc(`Number of Transcations`)), 10)
```
```{r}
# Selecting by Number of Transcations
names(Transactions_by_Country) <- c("Country", "Number of Transactions")

Transaction_by_Country_plot <- ggplot(head(Transactions_by_Country,5), aes(x = reorder(Country,-`Number of Transactions`), y = `Number of Transactions`)) + geom_bar(stat = 'identity', fill = "Steel Blue") +
  geom_text(aes(label = `Number of Transactions`)) +
  ggtitle('Top 5 Countries by Number of Transactions') + xlab('Countries') +
  ylab('Number of Transactions') +
  theme_minimal() 
print(Transaction_by_Country_plot)


#UK has the major portion of the customers with respect to other countries
```

```{r}
# the average value per order
retail_df %>% 
  mutate(Value = UnitPrice * Quantity) %>% 
  group_by(InvoiceNo) %>% 
  summarise(n = mean(Value)) %>%
  ggplot(aes(x=n)) +
  geom_histogram(bins = 200000, fill="firebrick3", colour = "sandybrown") + 
  coord_cartesian(xlim=c(0,100)) +
  scale_x_continuous(breaks=seq(0,100,10)) +
  labs(x = "Average Value per Purchase", y = "") + 
  theme_grey(base_size = 14)


#most of the orders have a value below £20
```
```{r}
corrplot(cor(retail_df), type = 'upper', method = 'number', tl.cex = 0.9)
```
# 4 Modelling

# Scaling Data
```{r}
#Sacling the data
RFM <- data.frame(RFM)
row.names(RFM) <- RFM$CustomerID
RFM <- RFM[,-1]
RFM_scaled <- scale(RFM) 
RFM_scaled <- data.frame(RFM_scaled)
```

### Determining Optimal Cluster
```{r}
#We used two most popular methods to find an optimal number of clusters:

# 1. Elbow Method
# 2. Silhouette Method
```

```{r}
# . Elbow method
fviz_nbclust(RFM_scaled, kmeans, method = "wss") + geom_vline(xintercept = 3, linetype = 2)

#from the graph, cluster 3 is where the curve starts to bend, so we picked k=3 as the optimal cluster.
```

```{r}
#2. Average Silhoute Method

#the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k Square.
```

```{r}
fviz_nbclust(RFM_scaled, kmeans, method = "silhouette")

#From the graph k=4 is the Optimal number of Cluster and k=3 is the next best.
```

## 4.1 K-Means Clustering
```{r}
#visualize kmeans clusters using both k=3 and k=4 for better understanding.
k3 <- kmeans(RFM_scaled, centers = 3, nstart = 25)
k4 <- kmeans(RFM_scaled, centers = 4, nstart = 25)

fviz_cluster(k3, geom = "point", data = RFM_scaled, pointsize = 0.2) + ggtitle("k = 3")
```

```{r}
fviz_cluster(k4, geom = "point", data = RFM_scaled, pointsize = 0.2) + ggtitle("k = 4")

#We noted that there are some overlapping of clusters for k=4, hence we confirm that k=3 is the best and optimal k.
```

```{r}
#After Comparing the algorithms we decided that K=3 is the optimal Cluster.
#summary statistics of each cluster for each of the variables.

res <- cbind(RFM, ClusterId = k3$cluster)
res <- as.data.frame(res)
```

```{r}
#Visualize the Frequency K-means clutering
ggplot(res, aes(x = ClusterId, y = Frequency, group = ClusterId, fill = as.factor(ClusterId))) + 
  geom_boxplot(show.legend = FALSE) + theme_minimal() + scale_fill_brewer(palette = "Set2") 
```

```{r}
#Visualize the Moenatry K-means clutering
ggplot(res, aes(x = ClusterId, y = Monetary, group = ClusterId, fill = as.factor(ClusterId))) + 
  geom_boxplot(show.legend = FALSE) + theme_minimal() + scale_fill_brewer(palette = "Set2")
```

```{r}
##Visualize the Recency K-means clutering
ggplot(res, aes(x = ClusterId, y = Recency, group = ClusterId, fill = as.factor(ClusterId))) + 
  geom_boxplot(show.legend = FALSE) + theme_minimal() + scale_fill_brewer(palette = "Set2")
```


## 4.2 Hierachical Clustering
```{r}
#Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. It does not require one to pre-specify the number of clusters to be generated as is required by the k-means approach.

#Furthermore, hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations; the dendrogram.
```

```{r}
#Determining Optimal Cluster
# 1. Elbow Method
fviz_nbclust(RFM_scaled, FUN = hcut, method = "wss") + geom_vline(xintercept = 3, linetype = 2)

#The optimal cluster is k=3
```

```{r}
#2. Average Silhouette Method

fviz_nbclust(RFM_scaled, FUN = hcut, method = "silhouette")

#we've note that k=2 is the Optimal number of Cluster and k=3 is the next best
```

```{r}
#Hierarchical Clustering

#We perform agglomerative HC with hclust. To begin we computed the dissimilarity values with dist and then fed the values into hclust and specify the agglomeration method to be used.
```

```{r}
euclidian_dist <- dist(RFM_scaled, method = "euclidean")
```

```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(euclidian_dist, method = "single" )

hc2 <- hclust(euclidian_dist, method = "complete" )

hc3 <- hclust(euclidian_dist, method = "ward.D2" )

hc4 <- hclust(euclidian_dist, method = "average" )

m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
```

```{r}
# function to compute coefficient
ac <- function(x) {
  agnes(RFM_scaled, method = x)$ac
}

map_dbl(m, ac)
```

```{r}
#The agnes$ac value gets the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).
```

```{r}
#visualize complete(h2) dendrogram

hc2 <- as.dendrogram(hc2)
cd = color_branches(hc2,k = 3)
plot(cd)
```

```{r}
#visualize ward(h3) dendrogram
hc3 <- as.dendrogram(hc3)
cd = color_branches(hc3,k = 3)
plot(cd)
```


```{r}
#We observed that the Complete linkage creates clusters for each outliers and thus creates 2 clusters each for 2 outlier which would not provide good result. therefore proceeded to with Ward’s method.
```

```{r}
#Summar Statistic
ward.clust = cutree(hc3,k = 3)
res1 <- cbind(RFM, ClusterId = ward.clust)
res1 <- as.data.frame(res1)
```

```{r}
#Visualization for the Frequency clusters
ggplot(res1, aes(x = ClusterId, y = Frequency, group = ClusterId, fill = as.factor(ClusterId))) + 
  geom_boxplot(show.legend = FALSE) + theme_minimal() + scale_fill_brewer(palette = "Set2")
```

```{r}
#Visualization for the Monetry clusters
ggplot(res1, aes(x = ClusterId, y = Monetary, group = ClusterId, fill = as.factor(ClusterId))) + 
  geom_boxplot(show.legend = FALSE) + theme_minimal() + scale_fill_brewer(palette = "Set2")
```

```{r}
#Visualization for the Recency clusters
ggplot(res1, aes(x = ClusterId, y = Recency, group = ClusterId, fill = as.factor(ClusterId))) + 
  geom_boxplot(show.legend = FALSE) + theme_minimal() + scale_fill_brewer(palette = "Set2")
```


# 5. Conclusion
```{r}
#Although both the methods did not give too good results, but k-means Clustering provided better results for this dataset.
```

```{r}
#K-means Clustering results
aggregate(res,by = list(res$ClusterId),FUN = mean)
```
K-Means Clustering with 3 Clusters:

Customers in Cluster 1 are the customers with high amount of transactions, are frequent buyers, and recent buyers as compared to other customers, hence most important from business point of view.

Customers in Cluster 2 are the customers with average amount of transactions as compared to other customers.

Customers in Cluster 3 are the customers with least amount of transactions, are infrequent buyers, and not recent buyers and hence least of importance from business point of view.

```{r}
#Hierarchical clustering results
aggregate(res1,by = list(res1$ClusterId),FUN = mean)
```
Hierarchical Clustering with 3 Clusters

Customers in Cluster 1 are the customers with average amount of transactions as compared to other customers.

Customers in Cluster 2 are the customers with least amount of transactions, are infrequent buyers, and not recent buyers and hence least of importance from business point of view.

Customers in Cluster 3 are the customers with high amount of transactions, are frequent buyers, and recent buyers as compared to other customers, hence most important from business point of view.

We note that all the four linkage methods are quite similar and close to 1, but Ward’s method gives the best result.
```
```r
```{r}
#getting the dataset
wrapper <- RFM
```

```{r}
# checking the dataset
head(wrapper)
```

```{r}
#selecting the numerical data
wrapper_num <- wrapper[]
```
```{r}
# normalising the data


```


